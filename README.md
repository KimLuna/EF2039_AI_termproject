# ðŸŽµ AI Vocal Separator

### **Term Project 01 â€“ AI Model to User Application**

**Author: Seoyeong Kim (20240932)**

---

## ðŸ“Œ 1. Purpose

This project follows the full development cycle required by **Term Project 01 â€“ AI Model to User Application**, including:

* AI idea development
* Pipeline design
* Model selection (pretrained model from open-source code)
* Environment setup
* Model analysis
* CLI â†’ Web application development
* Visualization using signal processing tools
* Git-based code management
* Distribution & documentation
* Demo video for advertisement

The goal is to understand *how an AI model evolves into a real-world user application*.

---

## ðŸš€ 2. Motivation & Background

### **Motivation**

I observed many people using Appleâ€™s **Sing** (vocal-removal) feature, but:

* It does **not** work for every song
* It cannot separate individual instruments
* Creators and musicians want more flexible and accurate tools

This motivated me to build an **AI-powered vocal/instrument separator** that works for any MP3 file.

### **Model Selection â€“ Spleeter**

I selected **Spleeter (by Deezer Research)** because:

* Open-source
* Pretrained U-Net architecture
* High-quality separation (2/4/5 stems)
* Supports both CLI and Python API
* Industry-level performance

---

## ðŸ§© 3. Project Pipeline (Simplified)

```
User Upload â†’ Flask Backend â†’ Librosa Preprocessing
       â†“
Spleeter Model (2/4/5 stems)
       â†“
Separated Stems (WAV)
       â†“
Generate Waveform & Spectrogram Images
       â†“
Return Results to Browser
```

---

## ðŸ§  4. Model Analysis Summary

### **1) STFT (Short-Time Fourier Transform)**

Converts waveform (time domain) â†’ spectrogram (frequency domain)

### **2) U-Net Architecture**

* Encoder: extracts features
* Bottleneck: learns instrument-specific patterns
* Decoder: reconstructs masks
* Skip connections: reduce loss of information

### **3) Soft Masking + Inverse STFT**

Applies predicted masks â†’ reconstructs each stem â†’ outputs WAV files.

---

## ðŸ–¥ 5. System Requirements

### âœ” Python Version

* **Python 3.8 ~ 3.10 supported**
* Python 3.11+ **NOT supported** (Spleeter incompatible)

### âœ” FFmpeg required (audio I/O)

Ubuntu:

```bash
sudo apt-get install ffmpeg
```

Windows:

```bash
choco install ffmpeg
```

### âœ” Install dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸ’» 6. Usage

### **CLI Version**

Run default (2 stems):

```bash
python main.py
```

Choose model:

```bash
python main.py -m 4stems
```

Choose MP3 file:

```bash
python main.py -i my_song.mp3
```

---

### **Web Version (Flask)**

Run server:

```bash
python app.py
```

Open browser:

```
http://localhost:8000
```

Upload MP3 â†’ choose stem model â†’ separate.

---

## ðŸ“‚ 7. Output Structure

```
output/
 â”œâ”€â”€ test_song/
 â”‚   â”œâ”€â”€ vocals.wav
 â”‚   â”œâ”€â”€ accompaniment.wav
 â”‚   â”œâ”€â”€ drums.wav
 â”‚   â”œâ”€â”€ bass.wav
 â”‚   â””â”€â”€ other.wav
```

---

## ðŸ“Š Example Visualization Outputs

Below are sample **waveform** and **spectrogram** images automatically generated by the app using `librosa` and saved in `static/` during processing.

### ðŸŽ¼ Waveform Example
`static/waveforms/piano_wave.png`

<img src="static/waveforms/piano_wave.png" width="500">


### ðŸ”¥ Spectrogram Example
`static/spectrograms/bass_spec.png`

<img src="static/spectrograms/bass_spec.png" width="500">


---

## ðŸŽ¥ Demo Video

`demo.mp4`



---

## ðŸ“¦ 10. Code Structure

```
EF2039_AI_TERMPROJECT/
 â”œâ”€â”€ app.py
 â”œâ”€â”€ main.py
 â”œâ”€â”€ requirements.txt
 â”œâ”€â”€ templates/
 â”‚   â””â”€â”€ index.html
 â”œâ”€â”€ static/
 â”‚   â”œâ”€â”€ waveforms/
 â”‚   â””â”€â”€ spectrograms/
 â”œâ”€â”€ output/
 â”œâ”€â”€ uploads/
 â””â”€â”€ README.md
```

---


## ðŸ”® 11. Future Extensions

* Fine-tuning for **engineering-frequency signal analysis**
* Instrument-wise **sheet music generation**
* Real-time vocal removal (streaming)
* AI-assisted music arrangement tools
* Acoustic fingerprinting research


